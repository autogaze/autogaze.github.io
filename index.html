<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AutoGaze</title>
    <meta name="description" content="AutoGaze: Attend Before Attention - Efficient and Scalable Video Understanding via Autoregressive Gazing">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="AutoGaze" property="og:title">
    <meta content="Efficient and Scalable Video Understanding via Autoregressive Gazing" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:description" content="AutoGaze: Efficient and Scalable Video Understanding via Autoregressive Gazing">
    <meta name="twitter:image:src" content="assets/figures/autogaze/teaser_v21.png">

    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/flickity@2/dist/flickity.min.css">
    <script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script>
    <script src="assets/scripts/navbar.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', 'Helvetica Neue', Arial, sans-serif"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
    <div class="container blog" id="first-content" style="background: linear-gradient(180deg, var(--gradient-start), var(--gradient-end));">
        <div class="blog-title">
            <div class="blog-intro">
                <div class="title-column">
                    <h1 class="title">Attend Before Attention: <br>Efficient and Scalable Video Understanding<br> via Autoregressive Gazing</h1>
<!--                    <br>-->
                    <div class="author-list">
                        <div class="author-item"><a href="https://bfshi.github.io/" target="_blank">Baifeng Shi</a>*<sup>1,4</sup></div>
                        <div class="author-item"><a href="https://stephanie-fu.github.io/" target="_blank">Stephanie Fu</a>*<sup>1</sup></div>
                        <div class="author-item"><a href="https://tonylian.com/" target="_blank">Long Lian</a><sup>1</sup></div>
                        <div class="author-item"><a href="https://sites.google.com/site/yhrspace/" target="_blank">Hanrong Ye</a><sup>4</sup></div>
                        <div class="author-item"><a href="https://deigen.net/" target="_blank">David Eigen</a><sup>3</sup></div>
                        <div class="author-item"><a href="https://scholar.google.com/citations?user=_qdnxtsAAAAJ&hl=en" target="_blank">Aaron Reite</a><sup>3</sup></div>
                        <div class="author-item"><a href="https://sites.google.com/site/boyilics/home" target="_blank">Boyi Li</a><sup>1,4</sup></div>
                        <div class="author-item"><a href="https://jankautz.com/" target="_blank">Jan Kautz</a><sup>4</sup></div>
                        <div class="author-item"><a href="https://hanlab.mit.edu/songhan" target="_blank">Song Han</a><sup>2,4</sup></div>
                        <div class="author-item"><a href="https://dchan.cc/" target="_blank">David M. Chan</a><sup>†1</sup></div>
                        <div class="author-item"><a href="https://pmolchanov.com/" target="_blank">Pavlo Molchanov</a><sup>†4</sup></div>
                        <div class="author-item"><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a><sup>†1</sup></div>
                        <div class="author-item"><a href="https://hongxu-yin.github.io/" target="_blank">Hongxu Yin</a><sup>†4</sup></div>
                    </div>
                    <br><br>
                    <div class="author-meta">
                        <p class="author">
                            <sup>1</sup>UC Berkeley &nbsp; <sup>2</sup>MIT &nbsp; <sup>3</sup>Clarifai &nbsp; <sup>4</sup>NVIDIA
                        </p>
                        <p class="author">
                            <sup>*</sup>Equal contribution &nbsp;&nbsp; <sup>†</sup>Equal advising
                        </p>
                    </div>
                </div>
                <div class="info">
                    <div>
                        <a href="#" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)">Paper <i class="fa-solid fa-book-open"></i></a>
                        <a href="#" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)">Code <i class="fa-solid fa-code"></i></a>
                        <a href="https://huggingface.co/datasets/bfshi/HLVid" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)" target="_blank">HLVid Benchmark <i class="fa-solid fa-database"></i></a>
                        <a href="#" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)">Demo <i class="fa-solid fa-laptop-code"></i></a>
                    </div>
                </div>

                <div class="abstract-column">
                    <p class="abstract">
                        <b>AutoGaze</b> is a lightweight module that removes redundant video patches before being processed downstream by a ViT or MLLM. Trained with next-token prediction and reinforcement learning, AutoGaze autoregressively <b>selects a minimal set of patches that reconstructs the video, eliminating redundancy while preserving information</b>. Empirically, AutoGaze reduces visual tokens by 4-100× and accelerates ViTs and MLLMs by up to 19×, enabling scaling MLLMs to 1K-frame 4K-resolution videos and achieving superior results on video benchmarks.
                    </p>
                    <p class="abstract">
                        We also introduce <b>HLVid: the first high-resolution, long-form video QA benchmark</b>. On HLVid, an MLLM scaled with AutoGaze outperforms the previous SOTA MLLM by 6.3%.
                    </p>
                </div>
            </div>
        </div>
    </div>

    <div class="container blog main">
        <video controls src="./assets/figures/video.mp4" class="teaser_video" title="AutoGaze: Efficient and Scalable Video Understanding via Autoregressive Gazing"></video>
        <p class="video-intro">watch this for a quick intro!</p>
    </div>

    <div class="container blog main">
        <h1>Quickstart</h1>
        <p>
            See <a href="https://github.com/stephanie-fu/gengaze/blob/release/QUICK_START.md">QUICK_START.md</a> in the codebase for the full guide.
        </p>
        <br>
        <pre><code class="python">import torch
from autogaze.datasets.video_utils import transform_video_for_pytorch
from autogaze.models.autogaze import AutoGazeImageProcessor, AutoGaze

autogaze_transform = AutoGazeImageProcessor.from_pretrained("bfshi/AutoGaze")
autogaze_model = AutoGaze.from_pretrained("bfshi/AutoGaze")

# ...prepare video input...

video_input_autogaze = transform_video_for_pytorch(raw_video, autogaze_transform)[None]

with torch.inference_mode():
    gaze_outputs = autogaze_model({"video": video_input_autogaze},
                                  gazing_ratio=0.75,
                                  task_loss_requirement=0.7)
</code></pre>
        <br>
    </div>
    <div class="container blog main first" id="blog-main">
        <h1>Introduction</h1>
        <p class="text">
            When observing a moving scene, humans don't process every detail equally. Our eyes dart around to moving objects, capture fine details, and skip over static backgrounds, efficiently understanding scenes by selectively attending to informative regions. This allows us to process high-FPS, high-resolution video streams in real time.
        </p>

        <p class="text">
            In contrast, modern video understanding models still process every pixel in every frame equally, <b>wasting computation due to spatiotemporal redundancy in videos</b>. Thus, these models cannot scale to <em>long-form</em> and <em>high-resolution</em> videos crucial for real-world applications due to computational cost.
        </p>

        <p class="text highlight-box">
            <b>We propose AutoGaze, a lightweight model that attends to informative patches and removes redundant ones before passing into a ViT or MLLM</b>. AutoGaze perceives each frame and autoregressively selects a minimal set of multi-scale patches which, along with the selected patches from previous frames, can reconstruct the current frame.
        </p>
    </div>
    <div class="container blog main">
        <img src="assets/figures/autogaze/teaser_v21.png" style="width: 100%;">
        <p class="caption">
            <b>Left: </b> Existing MLLMs either <span id="naive-mllm-label">process all pixels which is inefficient</span>, or <span id="prune-mllm-label">prune tokens only in their LLMs, leaving ViTs the computational bottleneck</span>. In contrast, <span id="autogaze-label">AutoGaze eliminates redundant patches by up to 100× <em>before</em> ViTs, accelerating ViTs and MLLMs by up to 19×</span>.
            <br><br>
            <b>Right: </b> This efficiency enables MLLMs with AutoGaze to <b>scale to 1K-frame, 4K-resolution videos and achieve superior performance</b> on HLVid, our new long, high-resolution video benchmark, surpassing prior MLLMs limited to short or low-resolution videos.
        </p>
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
    <div class="container blog main">
        <h1>Architecture and Training</h1>
        <p class="text">
            Given an input video, <span class="inline-highlight" data-figure="arch-fig" data-regions="region-middle">AutoGaze selects a minimal set of patches which reconstructs the video</span> within some user-specified reconstruction loss threshold or patch budget.
            The model is made up of a convolutional encoder and autoregressive transformer decoder, totaling 3M parameters. The decoder's vocabulary includes <span class="inline-highlight" data-figure="arch-fig" data-regions="region-left">patches from four scales</span>, letting it select different scales for regions with different levels of detail and reducing patches while preserving reconstruction quality.
        </p>
<!--    <div class="container blog main">-->
<!--        <h2>Training Pipeline</h2>-->
        <p class="text">
            <span class="inline-highlight" data-figure="arch-fig" data-regions="region-right">AutoGaze is trained in two stages:</span> we first pre-train with next-token prediction (NTP) on ground-truth gazing sequences, then further post-train AutoGaze using RL with reconstruction reward to discover gazing sequences with lower reconstruction loss.
        </p>
        <br>
        <div class="figure-wrapper" id="arch-fig">
            <img src="assets/figures/autogaze/architecture_v15.png" style="width: 100%;">
            <svg class="fig-dim-svg" xmlns="http://www.w3.org/2000/svg">
                <rect class="region-border" data-region="region-left"   x="1%"   y="1%" width="22%"  height="98%" fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
                <rect class="region-border" data-region="region-middle" x="24%"  y="1%" width="58%"  height="98%" fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
                <rect class="region-border" data-region="region-right"  x="83%"  y="1%" width="17%"  height="98%" fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
            </svg>
        </div>
<!--        <p class="caption">-->
<!--            <b>Left & Middle: </b> Given a video, AutoGaze processes each frame and outputs indices of multi-scale patches based on the history of frames and selected patches. Once AutoGaze believes the previously-gazed patches are sufficient to reconstruct the current frame, it automatically stops gazing and moves to the next frame.-->
<!--        </p>-->
<!--        <p class="caption">-->
<!--        <b>Right: </b> AutoGaze is trained in two stages: next-token-prediction pre-training on collected gazing sequences, and RL post-training with reconstruction reward.-->
<!--        </p>-->
<!--    </div>-->
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
    <div class="container blog main">
    <h1>Qualitative Results</h1>
        <p class="text">
            For each example below, we show the original video, gazed patches from two scales, and reconstructed video.
        </p>
        <p class="text">
            In general, AutoGaze can <span class="inline-highlight" data-figure="results-fig" data-regions="panel-a,panel-b,panel-c,panel-d,panel-e">1) focus on moving objects while removing redundancy in static regions</span>, <span class="inline-highlight" data-figure="results-fig" data-regions="panel-c,panel-e">2) adapt to scene changes</span> by selecting more patches, and <span class="inline-highlight" data-figure="results-fig" data-regions="panel-c,panel-f">3) use different scales</span> based on level of detail. This allows AutoGaze to select a small ratio of patches (gazing ratio) without much information loss, as reflected by the reconstruction quality.
        </p>
        <br>
    <div class="figure-wrapper" id="results-fig">
        <img src="assets/figures/autogaze/main_results.png" style="width: 100%;">
        <svg class="fig-dim-svg" xmlns="http://www.w3.org/2000/svg">
            <rect class="region-border" data-region="panel-a" x="0.5%"    y="1%"   width="32.3%" height="48%"  fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
            <rect class="region-border" data-region="panel-b" x="33.8%"   y="1%"   width="32.3%" height="48%"  fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
            <rect class="region-border" data-region="panel-c" x="67.1%"   y="1%"   width="32.4%" height="48%"  fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
            <rect class="region-border" data-region="panel-d" x="0.5%"    y="51%"  width="32.3%" height="48%"  fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
            <rect class="region-border" data-region="panel-e" x="33.8%"   y="51%"  width="32.3%" height="48%"  fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
            <rect class="region-border" data-region="panel-f" x="67.1%"   y="51%"  width="32.4%" height="48%"  fill="transparent" stroke="#e1b76e" stroke-width="0" rx="12"/>
        </svg>
    </div>
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
<!--    <div class="container blog main">-->
<!--        <h1>What is AutoGaze Paying Attention To?</h1>-->
<!--        <p class="text">-->
<!--            AutoGaze's efficiency comes from selecting only a small fraction of patches — but does it make principled decisions about which patches to select and at what scale?-->
<!--        </p>-->
<!--    </div>-->
<!--    <div class="container blog main two-column-figure">-->
<!--        <div class="text-column">-->
<!--            <h2>AutoGaze Targets Moving Patches</h2>-->
<!--            <p class="text">-->
<!--                Motion is a primary source of new information across video frames, and intuitively should be selected. We indeed find that across all scales, AutoGaze more frequently selects patches with higher optical flow.-->
<!--            </p>-->
<!--            <p class="text">-->
<!--                <b>Left: </b> AutoGaze uses coarser scales to capture higher optical flow.-->
<!--                <br>-->
<!--                <b>Right: </b> Across all scales, AutoGaze more frequently selects patches with higher optical flow. Error bars represent SEM.-->
<!--            </p>-->
<!--        </div>-->
<!--        <div class="figure-column">-->
<!--            <img src="assets/figures/autogaze/opticalflow_4.png">-->
<!--        </div>-->
<!--    </div>-->

<!--    <div class="container blog main two-column-figure">-->
<!--        <div class="text-column">-->
<!--            <h2>Gazing Scale Correlates with Patch Detail</h2>-->
<!--            <p class="text">-->
<!--                Regions with different levels of detail should be represented with different scales.-->
<!--            </p>-->
<!--            <p class="text">-->
<!--                <b>Left: </b> At finer scales, AutoGaze selects more detailed patches (measured as Laplacian variance).-->
<!--                <br>-->
<!--                <b>Right: </b> With increasing detail, AutoGaze uses finer scales (ρ = .12, p < 0.001). Error bars = SEM.-->
<!--            </p>-->
<!--        </div>-->
<!--        <div class="figure-column">-->
<!--            <img src="assets/figures/autogaze/texture_stats.png">-->
<!--        </div>-->
<!--    </div>-->

    <div class="container blog main">
        <h1>Generalization to Out-of-Distribution Videos</h1>
        <p class="text">
            AutoGaze successfully generalizes to unconventional scenarios including CCTV footage, robot videos, and videos with constantly swapping foreground objects.
        </p>
    </div>

    <div class="container blog main">
        <div class="ood-carousel-container">
            <div class="ood-flickity">
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/autogaze_ood_semantics_1.png" alt="OOD Semantics - CCTV">
                </div>
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/autogaze_ood_semantics_2.png" alt="OOD Semantics - Robot">
                </div>
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/autogaze_ood_semantics_3.png" alt="OOD Semantics - Comparison">
                </div>
            </div>
        </div>
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
    <div class="container blog main">
        <h1>Generalization to Videos with Camera Motion</h1>
        <p class="text">
            AutoGaze also generalizes to videos with camera motion. Although the gazing pattern is less intuitive due to the global motion,
            the gazing ratio remains similar and the reconstruction quality is still high.
        </p>
        <video controls src="./assets/figures/camera_motion.mp4" class="wide_video"></video>
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
    <div class="container blog main">
        <h1>Efficiency of ViTs and MLLMs with AutoGaze</h1>
        <h2>How many patches do we need?</h2>

        <p class="text">
            There is a trade-off between gazing ratio and reconstruction loss: videos with higher FPS or resolution need lower gazing ratio to reach the same reconstruction quality.
            We find that a reconstruction loss threshold of 0.7 usually leads to <b><0.5% performance drop while allowing videos to be represented with 4×-100× fewer patches</b>.
        </p>
    </div>

    <div class="container blog main">
        <img src="assets/figures/autogaze/loss_recon_cut.png" style="width: 100%">
    </div>

    <div class="container blog main">
        <h2>What speedups do we get?</h2>
        <p class="text">
            When using the gazing ratio required for a reconstruction loss of 0.7, <b>AutoGaze achieves up to 19× and 10× speedup for ViTs and MLLMs, enabling scaling to 4K resolution</b>. Below, we show latency plots for one second of video with varying FPS and resolution.
        </p>
    </div>

    <div class="container blog main">
        <div class="latency-carousel-container">
            <div class="latency-flickity">
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/latency_vision.png" alt="Vision Latency">
                </div>
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/latency_llm.png" alt="LLM Latency">
                </div>
            </div>
        </div>
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
    <div class="container blog main">
        <h1>HLVid: High-Resolution, Long-Form Video Benchmark</h1>
        <p class="text">
            Although AutoGaze enables efficient understanding of long, high-resolution videos, <b>benchmarks to evaluate this capability are still missing</b>. We propose HLVid, the first long-form, high-resolution video QA benchmark featuring <b>300 QA pairs on 5-minute, 4K-resolution videos</b>. Each question is manually reviewed to ensure high resolution and understanding throughout the entire video is required.
        </p>
        <p class="text">
            <a href="">Check out and use HLVid on HuggingFace!</a>
        </p>
    </div>

    <div class="container blog main">
        <div class="hlvid-carousel-container">
            <div class="hlvid-flickity">
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/hlvid1.png" alt="HLVid Example 1">
                </div>
                <div class="carousel-cell">
                    <img src="assets/figures/autogaze/hlvid2.png" alt="HLVid Example 2">
                </div>
            </div>
        </div>
    </div>
    <div class="container blog main">
        <hr class="section-divider">
    </div>
    <div class="container blog main">
        <h1>Performance on Video Benchmarks</h1>
        <p class="text">
            We scale NVILA-8B to 1K-frame 4K-resolution videos, demonstrating consistent improvements on various benchmarks and outperforming strong MLLMs such as Qwen2.5-VL. See <a href="#">the paper</a> for detailed quantitative results comparing with other MLLMs.
        </p>
    </div>

    <div class="container blog main">
        <img src="assets/figures/autogaze/mllm_benchmarks.png" style="width: 100%;">
    </div>

    <div class="container blog main">
        <div class="table-wrapper">
            <table class="results-table">
                <thead>
                    <tr>
                        <th rowspan="2"></th>
                        <th rowspan="2">Max<br>#Frames</th>
                        <th rowspan="2">Max<br>Res.</th>
                        <th colspan="4"><em>general video</em></th>
                        <th colspan="3"><em>long video</em></th>
                        <th colspan="1"><em>high-res & long</em></th>
                    </tr>
                    <tr>
                        <th>VideoMME<br>(w/o sub)</th>
                        <th>VideoMME<br>(w/ sub)</th>
                        <th>MVBench<br>(test)</th>
                        <th>NExT-QA<br>(mc)</th>
                        <th>L-VidBench<br>(val)</th>
                        <th>EgoSchema<br>(test)</th>
                        <th>MLVU<br>(m-avg)</th>
                        <th>HLVid<br>(test)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="gradient-light">
                        <td>Gemini 1.5-Pro</td>
                        <td>-</td>
                        <td>-</td>
                        <td>75.0</td>
                        <td>81.3</td>
                        <td>60.5</td>
                        <td>-</td>
                        <td>64.0</td>
                        <td>71.2</td>
                        <td>-</td>
                        <td>-</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>Gemini 2.5 Flash-Lite</td>
                        <td>-</td>
                        <td>-</td>
                        <td>65.0</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>69.3</td>
                        <td>52.2</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>GPT-4o</td>
                        <td>-</td>
                        <td>-</td>
                        <td>71.9</td>
                        <td>77.2</td>
                        <td>64.6</td>
                        <td>-</td>
                        <td>66.7</td>
                        <td>72.2</td>
                        <td>64.6</td>
                        <td>49.3</td>
                    </tr>
                    <tr class="separator-row">
                        <td colspan="11"></td>
                    </tr>
                    <tr class="gradient-light">
                        <td>LLaVA-OV-8B</td>
                        <td>32</td>
                        <td>384</td>
                        <td>58.2</td>
                        <td>61.5</td>
                        <td>56.7</td>
                        <td>79.4</td>
                        <td>56.5</td>
                        <td>60.1</td>
                        <td>64.7</td>
                        <td>41.1</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>LongVILA-7B</td>
                        <td>2048</td>
                        <td>384</td>
                        <td>60.1</td>
                        <td>65.1</td>
                        <td>67.1</td>
                        <td>80.7</td>
                        <td>57.1</td>
                        <td>-</td>
                        <td>-</td>
                        <td>41.4</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>LongVILA-R1-7B</td>
                        <td>8192</td>
                        <td>448</td>
                        <td>65.1</td>
                        <td>71.1</td>
                        <td>-</td>
                        <td>81.5</td>
                        <td>58.0</td>
                        <td>-</td>
                        <td>-</td>
                        <td>42.2</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>Apollo-7B</td>
                        <td>2FPS</td>
                        <td>384</td>
                        <td>61.3</td>
                        <td>63.3</td>
                        <td>-</td>
                        <td>-</td>
                        <td>58.5</td>
                        <td>-</td>
                        <td>70.9</td>
                        <td>-</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>VideoLLaMA3-7B</td>
                        <td>180</td>
                        <td>384</td>
                        <td>66.2</td>
                        <td>70.3</td>
                        <td>69.7</td>
                        <td><strong>84.5</strong></td>
                        <td>59.8</td>
                        <td>63.3</td>
                        <td>73.0</td>
                        <td>38.8</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>VideoChat-Flash</td>
                        <td>10000</td>
                        <td>448</td>
                        <td>65.3</td>
                        <td>69.7</td>
                        <td><strong>74.0</strong></td>
                        <td>-</td>
                        <td><strong>64.7</strong></td>
                        <td>-</td>
                        <td><strong>74.7</strong></td>
                        <td>46.6</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>InternVL3.5-8B</td>
                        <td>64</td>
                        <td>448</td>
                        <td>66.0</td>
                        <td>68.6</td>
                        <td>72.1</td>
                        <td>-</td>
                        <td>62.1</td>
                        <td>-</td>
                        <td>70.2</td>
                        <td>39.9</td>
                    </tr>
                    <tr class="gradient-light">
                        <td>Qwen2.5-VL-7B</td>
                        <td>48</td>
                        <td>896</td>
                        <td>65.1</td>
                        <td>71.6</td>
                        <td>69.6</td>
                        <td>-</td>
                        <td>56.0</td>
                        <td>65.0</td>
                        <td>70.2</td>
                        <td>48.1</td>
                    </tr>
                    <tr class="separator-row">
                        <td colspan="11"></td>
                    </tr>
                    <tr class="gradient-medium">
                        <td>NVILA-8B-Video</td>
                        <td>256</td>
                        <td>448</td>
                        <td>64.2</td>
                        <td>70.0</td>
                        <td>68.1</td>
                        <td>82.2</td>
                        <td>57.7</td>
                        <td>-</td>
                        <td>70.1</td>
                        <td>42.5</td>
                    </tr>
                    <tr class="gradient-dark">
                        <td><strong>NVILA-8B-HD-Video</strong></td>
                        <td>1024</td>
                        <td>3584</td>
                        <td><strong>67.0</strong></td>
                        <td><strong>71.8</strong></td>
                        <td>69.7</td>
                        <td>82.8</td>
                        <td>61.0</td>
                        <td><strong>66.9</strong></td>
                        <td>71.6</td>
                        <td><strong>52.6</strong></td>
                    </tr>
                    <tr class="gradient-dark comparison-row">
                        <td><em>(vs. NVILA-8B-Video)</em></td>
                        <td class="improvement"><strong><em>(×4)</em></strong></td>
                        <td class="improvement"><strong><em>(×8)</em></strong></td>
                        <td class="improvement"><strong><em>(+2.8)</em></strong></td>
                        <td class="improvement"><strong><em>(+1.8)</em></strong></td>
                        <td class="improvement"><strong><em>(+1.6)</em></strong></td>
                        <td class="improvement"><strong><em>(+0.6)</em></strong></td>
                        <td class="improvement"><strong><em>(+3.3)</em></strong></td>
                        <td class="improvement"><strong><em>-</em></strong></td>
                        <td class="improvement"><strong><em>(+1.5)</em></strong></td>
                        <td class="improvement"><strong><em>(+10.1)</em></strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="container blog main">
        <h1>Citation</h1>
        <p class="text">
            If you find AutoGaze useful for your research, please cite:
        </p>
<pre><code class="plaintext">{



}</code></pre>
    </div>

    <!-- Footer Page -->
    <footer>
<!--        style="background: linear-gradient(180deg, var(&#45;&#45;gradient-end), var(&#45;&#45;gradient-start));">-->
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>.
            </p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="assets/scripts/main.js"></script>
    <script>
        // Dim rects keyed by active regions (matching data-regions exactly).
        // Each entry is an explicit array of {x,y,w,h} rects (% values) to shade,
        // or {type:'poly', points:[[x,y],...], rounded:[indices]} for L-shapes etc.
        // Individual and compound cases are defined separately so they can be tuned independently.
        var spanDimRects = {
            'arch-fig': {
                // active=middle → dim left (individual) + right (individual)
                'region-middle': [
                    {x: 1,    y: 0, w: 22.5,  h: 100},
                    {x: 82.5, y: 0, w: 16.25, h: 100}
                ],
                // active=left → dim middle+right as one seamless compound rect
                'region-left': [
                    {x: 23.5, y: 0, w: 76.5, h: 100}
                ],
                // active=right → dim left+middle as one seamless compound rect
                'region-right': [
                    {x: 1, y: 0, w: 82, h: 100}
                ]
            },
            'results-fig': {
                // active=a-e → dim only panel-f
                'panel-a,panel-b,panel-c,panel-d,panel-e': [
                    {x: 66.7, y: 50, w: 33.3, h: 50}
                ],
                // active=c,e → dim a+b+d as L-shape (rounded outside, sharp inside corners) + f rect
                'panel-c,panel-e': [
                    {type: 'poly',
                     points: [[0,0],[66.7,0],[66.7,50],[33.3,50],[33.3,100],[0,100]],
                     rounded: [0,1,4,5]},
                    {x: 66.7, y: 50, w: 33.3, h: 50}
                ],
                // active=c,f → dim a+b+d+e merged into left 2/3 tall rect
                'panel-c,panel-f': [
                    {x: 0, y: 0, w: 66.7, h: 100}
                ],
                // Individual panel hover dimming
                'panel-a': [
                    {x: 33.3, y: 0, w: 66.7, h: 100},  // right 2/3
                    {x: 0, y: 50, w: 33.3, h: 50}      // panel-d
                ],
                'panel-b': [
                    {x: 0, y: 0, w: 33.3, h: 100},     // left column (a+d)
                    {x: 66.7, y: 0, w: 33.3, h: 100},  // right column (c+f)
                    {x: 33.3, y: 50, w: 33.3, h: 50}   // panel-e
                ],
                'panel-c': [
                    {x: 0, y: 0, w: 66.7, h: 100},     // left 2/3 (a+b+d+e)
                    {x: 66.7, y: 50, w: 33.3, h: 50}   // panel-f
                ],
                'panel-d': [
                    {x: 0, y: 0, w: 100, h: 50},       // entire top row
                    {x: 33.3, y: 50, w: 66.7, h: 50}   // e+f
                ],
                'panel-e': [
                    {x: 0, y: 0, w: 33.3, h: 100},     // left column (a+d)
                    {x: 66.7, y: 0, w: 33.3, h: 100},  // right column (c+f)
                    {x: 33.3, y: 0, w: 33.3, h: 50}    // panel-b
                ],
                'panel-f': [
                    {x: 0, y: 0, w: 66.7, h: 100},     // left 2/3 (a+b+d+e)
                    {x: 66.7, y: 0, w: 33.3, h: 50}    // panel-c
                ]
            }
        };

        var DIM_RX = 12; // corner radius in pixels, matching border rects

        // Build an SVG path d-string for a polygon where only corners at `roundedIndices`
        // get a quadratic-bezier arc of radius `r` pixels; others are sharp.
        // `points` is array of [xPct, yPct], converted to px using svgW/svgH.
        function makeRoundedPolyPath(points, roundedIndices, svgW, svgH, r) {
            var n = points.length;
            // Convert to absolute px
            var px = points.map(function(p) {
                return [p[0] / 100 * svgW, p[1] / 100 * svgH];
            });
            var rounded = roundedIndices;
            function isRounded(i) { return rounded.indexOf(i) !== -1; }
            // For each vertex, compute the two trim points (where line meets arc tangent)
            function trimPt(cur, prev, next) {
                var dx1 = prev[0] - cur[0], dy1 = prev[1] - cur[1];
                var d1 = Math.sqrt(dx1*dx1 + dy1*dy1);
                var dx2 = next[0] - cur[0], dy2 = next[1] - cur[1];
                var d2 = Math.sqrt(dx2*dx2 + dy2*dy2);
                var trim = Math.min(r, d1/2, d2/2);
                return [
                    [cur[0] + dx1/d1*trim, cur[1] + dy1/d1*trim],
                    [cur[0] + dx2/d2*trim, cur[1] + dy2/d2*trim]
                ];
            }
            var d = '';
            for (var i = 0; i < n; i++) {
                var prev = px[(i - 1 + n) % n];
                var cur  = px[i];
                var next = px[(i + 1) % n];
                if (isRounded(i)) {
                    var tp = trimPt(cur, prev, next);
                    var enterPt = tp[0]; // point on edge coming into this corner
                    var exitPt  = tp[1]; // point on edge going out of this corner
                    if (i === 0) {
                        d += 'M ' + enterPt[0].toFixed(2) + ' ' + enterPt[1].toFixed(2) + ' ';
                    } else {
                        d += 'L ' + enterPt[0].toFixed(2) + ' ' + enterPt[1].toFixed(2) + ' ';
                    }
                    // Quadratic bezier: control point is the corner itself
                    d += 'Q ' + cur[0].toFixed(2) + ' ' + cur[1].toFixed(2) + ' '
                              + exitPt[0].toFixed(2) + ' ' + exitPt[1].toFixed(2) + ' ';
                } else {
                    // Sharp corner
                    if (i === 0) {
                        d += 'M ' + cur[0].toFixed(2) + ' ' + cur[1].toFixed(2) + ' ';
                    } else {
                        d += 'L ' + cur[0].toFixed(2) + ' ' + cur[1].toFixed(2) + ' ';
                    }
                }
            }
            // Close: line back to start
            // If vertex 0 was rounded, we need to close to its enterPt (which was the M point)
            // The path already closes naturally with Z
            d += 'Z';
            return d;
        }

        // Create a dim SVG element (rect or poly path) and return it.
        function createDimShape(svg, shape) {
            var NS = 'http://www.w3.org/2000/svg';
            var el;
            if (shape.type === 'poly') {
                var w = svg.clientWidth  || svg.getBoundingClientRect().width;
                var h = svg.clientHeight || svg.getBoundingClientRect().height;
                el = document.createElementNS(NS, 'path');
                el.setAttribute('d', makeRoundedPolyPath(shape.points, shape.rounded, w, h, DIM_RX));
            } else {
                el = document.createElementNS(NS, 'rect');
                el.setAttribute('x', shape.x + '%');
                el.setAttribute('y', shape.y + '%');
                el.setAttribute('width', shape.w + '%');
                el.setAttribute('height', shape.h + '%');
                el.setAttribute('rx', DIM_RX);
            }
            el.setAttribute('fill', 'rgba(255,255,255,0.7)');
            el.style.pointerEvents = 'none';
            el.style.opacity = '0';
            el.style.transition = 'opacity 0.2s ease';
            el.classList.add('dynamic-dim');
            // Trigger fade-in animation
            setTimeout(function() { el.style.opacity = '1'; }, 10);
            return el;
        }

        document.querySelectorAll('.inline-highlight[data-figure]').forEach(function(span) {
            var figId = span.dataset.figure;
            var activeRegions = span.dataset.regions;
            var figWrapper = document.getElementById(figId);
            var svg = figWrapper.querySelector('.fig-dim-svg');

            span.addEventListener('mouseenter', function() {
                svg.querySelectorAll('.dynamic-dim').forEach(function(el) { el.remove(); });
                var shapes = (spanDimRects[figId] || {})[activeRegions] || [];
                shapes.forEach(function(shape) {
                    var el = createDimShape(svg, shape);
                    svg.insertBefore(el, svg.firstChild);
                });
                var activeArr = activeRegions.split(',');
                figWrapper.querySelectorAll('.region-border').forEach(function(el) {
                    el.style.strokeOpacity = activeArr.indexOf(el.dataset.region) !== -1 ? '1' : '0';
                });
            });

            span.addEventListener('mouseleave', function() {
                svg.querySelectorAll('.dynamic-dim').forEach(function(el) { el.remove(); });
                figWrapper.querySelectorAll('.region-border').forEach(function(el) {
                    el.style.strokeOpacity = '0';
                });
            });
        });

        // Reverse hover: region -> text highlighting
        document.querySelectorAll('.region-border').forEach(function(regionRect) {
            var regionName = regionRect.dataset.region;
            var figWrapper = regionRect.closest('.figure-wrapper');
            var figId = figWrapper.id;
            var svg = figWrapper.querySelector('.fig-dim-svg');

            regionRect.addEventListener('mouseenter', function() {
                // Reset ALL regions in this figure first
                figWrapper.querySelectorAll('.region-border').forEach(function(el) {
                    el.style.strokeOpacity = '0';
                });

                // Show only this region's border
                regionRect.style.strokeOpacity = '1';

                // Add dimming rectangles for non-hovered regions
                svg.querySelectorAll('.dynamic-dim').forEach(function(el) { el.remove(); });
                var shapes = (spanDimRects[figId] || {})[regionName] || [];
                shapes.forEach(function(shape) {
                    var el = createDimShape(svg, shape);
                    svg.insertBefore(el, svg.firstChild);
                });

                // Highlight corresponding text spans
                document.querySelectorAll('.inline-highlight[data-figure="' + figId + '"]').forEach(function(span) {
                    var regions = span.dataset.regions.split(',');
                    if (regions.indexOf(regionName) !== -1) {
                        span.classList.add('region-hovered');
                    }
                });
            });

            regionRect.addEventListener('mouseleave', function() {
                // Hide the border
                regionRect.style.strokeOpacity = '0';
                // Fade out dimming rectangles
                svg.querySelectorAll('.dynamic-dim').forEach(function(el) {
                    el.style.opacity = '0';
                    setTimeout(function() { el.remove(); }, 200);
                });
                // Don't hide the entire SVG - let other regions stay hoverable

                // Remove highlight from text
                document.querySelectorAll('.inline-highlight.region-hovered').forEach(function(span) {
                    span.classList.remove('region-hovered');
                });
            });
        });

        // OOD Carousel - Flickity initialization
        var oodFlickity = new Flickity('.ood-flickity', {
            cellAlign: 'center',
            contain: true,
            prevNextButtons: true,
            pageDots: false,
            wrapAround: true,
            imagesLoaded: true
        });

        // Latency Carousel - Flickity initialization
        var latencyFlickity = new Flickity('.latency-flickity', {
            cellAlign: 'center',
            contain: true,
            prevNextButtons: true,
            pageDots: false,
            wrapAround: true,
            imagesLoaded: true
        });

        // HLVid Carousel - Flickity initialization
        var hlvidFlickity = new Flickity('.hlvid-flickity', {
            cellAlign: 'center',
            contain: true,
            prevNextButtons: true,
            pageDots: false,
            wrapAround: true,
            imagesLoaded: true
        });
    </script>
</body>
</html>