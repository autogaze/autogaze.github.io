<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AutoGaze: Efficient and Scalable Video Understanding</title>
    <meta name="description" content="AutoGaze: Attend Before Attention - Efficient and Scalable Video Understanding via Autoregressive Gazing">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="AutoGaze" property="og:title">
    <meta content="Efficient and Scalable Video Understanding via Autoregressive Gazing" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:description" content="AutoGaze: Efficient and Scalable Video Understanding via Autoregressive Gazing">
    <meta name="twitter:image:src" content="assets/figures/autogaze/teaser_v14.png">

    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', 'Helvetica Neue', Arial, sans-serif"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <style>
        /* Gradient colors */
        :root {
            --gradient-start: #f1e6d2;
            --gradient-end: #ffffff;
            --link-brown: #000000;
            --link-brown-hover: #775223;
        }

        /* Increase page margins */
        .container {
            padding-right: calc(10vw - (100vw - 100%)) !important;
            padding-left: 10vw !important;
        }

        @media screen and (min-width: 769px) {
            .container {
                padding-right: calc(12vw - (100vw - 100%)) !important;
                padding-left: 12vw !important;
            }
        }

        @media screen and (min-width: 992px) {
            .container {
                padding-right: calc(16vw - (100vw - 100%)) !important;
                padding-left: 16vw !important;
            }
        }

        @media screen and (min-width: 1200px) {
            .container {
                padding-right: calc(18vw - (100vw - 100%)) !important;
                padding-left: 18vw !important;
            }
        }

        @media screen and (min-width: 1600px) {
            .container {
                padding-right: calc(22vw - (100vw - 100%)) !important;
                padding-left: 22vw !important;
            }
        }

        @media screen and (min-width: 2000px) {
            .container {
                padding-right: calc(26vw - (100vw - 100%)) !important;
                padding-left: 26vw !important;
            }
        }

        @media screen and (min-width: 2400px) {
            .container {
                padding-right: calc(28vw - (100vw - 100%)) !important;
                padding-left: 28vw !important;
            }
        }

        @media screen and (min-width: 3200px) {
            .container {
                padding-right: calc(36vw - (100vw - 100%)) !important;
                padding-left: 32vw !important;
            }
        }

        /* Link colors */
        a {
            color: var(--link-brown) !important;
            text-decoration: underline;
            text-decoration-thickness: 0.5px;
            text-underline-offset: 1px;
            font-weight: 400 !important;
            transition: text-decoration-thickness 0.2s ease;
        }
        a:hover {
            color: var(--link-brown-hover) !important;
            text-decoration-thickness: 1.5px;
        }

        /* Two-column title layout */
        #first-content .blog-title {
            display: block !important;
            max-width: 1200px;
            margin: 0 auto;
        }

        #first-content .blog-intro {
            display: grid !important;
            grid-template-columns: 40% 58% !important;
            gap: 1% !important;
            padding: 6vh 0 !important;
            align-items: start;
        }

        #first-content .title-column {
            padding-right: 2vw;
        }

        #first-content .abstract-column {
            padding-left: 2vw;
        }

        #first-content .abstract-column p.abstract:first-child {
            margin-top: 0;
            padding-top: 0;
        }

        /* Add subtle horizontal line when stacked vertically */
        @media screen and (max-width: 768px) {
            #first-content .abstract-column::before {
                content: '';
                display: block;
                width: 100%;
                height: 1px;
                background: rgba(0, 0, 0, 0.15);
                margin-bottom: 3vh;
            }
        }

        #first-content h1.title {
            font-size: 26px !important;
            line-height: 1.3 !important;
            margin-bottom: 0;
        }

        #first-content p.author {
            line-height: 1.6 !important;
            margin-bottom: 1.5vh;
            font-size: 16px;
        }

        #first-content p.abstract {
            font-size: 16px !important;
            line-height: 1.7 !important;
            margin-bottom: 2vh;
        }

        #first-content .info {
            margin-top: 3vh;
        }

        /* Author list styling - using CSS Grid for responsive columns */
        #first-content .author-list {
            display: inline-grid;
            grid-template-columns: repeat(2, auto);
            gap: 0.4vh 3vw;
            margin: 2vh 0;
        }

        #first-content .author-list .author-item {
            font-size: calc(80%) !important;
            line-height: 2.0;
            white-space: nowrap;
            font-family: "Figtree", -apple-system, BlinkMacSystemFont, "Segoe UI", "Helvetica Neue", Arial, sans-serif;
        }

        #first-content .author-list .author-item a {
            font-weight: 300 !important;
            font-size: inherit !important;
            text-decoration: underline;
            text-decoration-thickness: 0.5px;
            text-underline-offset: 1px;
            transition: text-decoration-thickness 0.2s ease;
        }

        #first-content .author-list .author-item a:hover {
            text-decoration-thickness: 1.5px;
        }

        #first-content .author-list .author-item sup {
            font-size: 0.75em;
        }

        /* Keep 2 columns even on very wide screens */
        @media screen and (min-width: 1800px) {
            #first-content .author-list {
                grid-template-columns: repeat(2, auto);
            }
        }

        /* When layout stacks vertically */
        @media screen and (max-width: 768px) {
            #first-content .blog-intro {
                grid-template-columns: 1fr !important;
                gap: 4vh !important;
            }

            #first-content .title-column,
            #first-content .abstract-column {
                padding-left: 0;
                padding-right: 0;
            }

            /* On mobile with stacked layout, full width */
            #first-content .author-table {
                max-width: 100%;
            }
        }

        /* Very small screens: reduce to 1 or 2 columns via CSS grid */
        @media screen and (max-width: 500px) {
            #first-content .author-table {
                display: block;
            }

            #first-content .author-table tbody,
            #first-content .author-table tr {
                display: block;
            }

            #first-content .author-table td {
                display: inline-block;
                width: 48%;
                padding: 0.3vh 2% 0.3vh 0;
                white-space: normal;
            }
        }

        /* White background for all images */
        .container.blog.main img,
        .container.blog.gray img,
        .container.blog.extra-large img {
            background-color: white;
            padding: 10px;
        }
    </style>
</head>

<body>
    <div class="container blog" id="first-content" style="background: linear-gradient(180deg, var(--gradient-start), var(--gradient-end));">
        <div class="blog-title">
            <div class="blog-intro">
                <div class="title-column">
                    <h1 class="title">Attend Before Attention: Efficient and Scalable Video Understanding via Autoregressive Gazing</h1>
                    <div class="author-list">
                        <div class="author-item"><a href="https://bfshi.github.io/" target="_blank">Baifeng Shi</a>*<sup>1,4</sup></div>
                        <div class="author-item"><a href="https://stephanie-fu.github.io/" target="_blank">Stephanie Fu</a>*<sup>1</sup></div>
                        <div class="author-item"><a href="https://tonylian.com/" target="_blank">Long Lian</a><sup>1</sup></div>
                        <div class="author-item"><a href="https://sites.google.com/site/yhrspace/" target="_blank">Hanrong Ye</a><sup>4</sup></div>
                        <div class="author-item"><a href="https://deigen.net/" target="_blank">David Eigen</a><sup>3</sup></div>
                        <div class="author-item"><a href="https://scholar.google.com/citations?user=_qdnxtsAAAAJ&hl=en" target="_blank">Aaron Reite</a><sup>3</sup></div>
                        <div class="author-item"><a href="https://sites.google.com/site/boyilics/home" target="_blank">Boyi Li</a><sup>1,4</sup></div>
                        <div class="author-item"><a href="https://jankautz.com/" target="_blank">Jan Kautz</a><sup>4</sup></div>
                        <div class="author-item"><a href="https://hanlab.mit.edu/songhan" target="_blank">Song Han</a><sup>2,4</sup></div>
                        <div class="author-item"><a href="https://dchan.cc/" target="_blank">David M. Chan</a><sup>†1</sup></div>
                        <div class="author-item"><a href="https://pmolchanov.com/" target="_blank">Pavlo Molchanov</a><sup>†4</sup></div>
                        <div class="author-item"><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a><sup>†1</sup></div>
                        <div class="author-item"><a href="https://hongxu-yin.github.io/" target="_blank">Hongxu Yin</a><sup>†4</sup></div>
                    </div>
                    <p class="author" style="padding-top: 0px;">
                        <sup>1</sup>UC Berkeley &nbsp; <sup>2</sup>MIT &nbsp; <sup>3</sup>Clarifai &nbsp; <sup>4</sup>NVIDIA
                    </p>
                    <p class="author" style="padding-top: 0px; font-size: 14px;">
                        <sup>*</sup>Equal contribution &nbsp;&nbsp; <sup>†</sup>Equal advising
                    </p>
                </div>

                <div class="abstract-column">
                    <p class="abstract">
                        <b>AutoGaze</b> is a lightweight module that removes redundant video patches before being processed downstream by a ViT or MLLM. Trained with next-token prediction and reinforcement learning, AutoGaze autoregressively <b>selects a minimal set of patches that reconstructs the video within some error threshold, eliminating redundancy while preserving information</b>. Empirically, AutoGaze reduces visual tokens by 4-100× and accelerates ViTs and MLLMs by up to 19×, enabling scaling MLLMs to 1K-frame 4K-resolution videos and achieving superior results on video benchmarks.
                    </p>
                    <p class="abstract">
                        We also introduce <b>HLVid</b>: the first high-resolution, long-form video QA benchmark with multi-minute 4K videos. On HLVid, an MLLM scaled with AutoGaze outperforms the previous SOTA MLLM by 6.3%.
                    </p>
                    <div class="info">
                        <div>
                            <a href="#" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)">Paper <i class="fa-solid fa-book-open"></i></a> &nbsp;&nbsp;
                            <a href="#" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)">Code <i class="fa-solid fa-code"></i></a>  &nbsp;&nbsp;
                            <a href="https://huggingface.co/datasets/bfshi/HLVid" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)" target="_blank">HLVid Benchmark <i class="fa-solid fa-database"></i></a> &nbsp;&nbsp;
                            <a href="#" class="button icon" style="background-color: rgba(0, 0, 0, 0.05)">Demo <i class="fa-solid fa-laptop-code"></i></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container blog extra-large">
        <video controls src="./assets/figures/video.mp4" class="teaser_video" title="AutoGaze: Efficient and Scalable Video Understanding via Autoregressive Gazing"></video>
        <p class="video-intro">watch this for a quick intro!</p>
    </div>

    <div class="container blog main">
        <h1>Quickstart</h1>
        <p>
            See <a href="https://github.com/stephanie-fu/gengaze/blob/release/QUICK_START.md">QUICK_START.md</a> in the codebase for the full guide.
        </p>
        <br>
        <pre><code class="python">import torch
from transformers import VivitImageProcessor
from autogaze.models.autogaze import AutoGaze

autogaze_model = AutoGaze.from_pretrained("bfshi/AutoGaze")
autogaze_transform = VivitImageProcessor.from_pretrained("facebook/vit-mae-large",
                                              size=autogaze_model.scales[-1],
                                              crop_size=autogaze_model.scales[-1])
# ...prepare video input...

with torch.inference_mode():
    gaze_outputs = autogaze_model({"video": video_input_autogaze},
                      gazing_ratio=0.75,
                      task_loss_requirement=0.7)</code></pre>
        <br>
        <pre><code class="python"># Use HLVid: 4K, long video benchmark
from datasets import load_dataset
ds = load_dataset("------")</code></pre>
    </div>
    <div class="container blog main first" id="blog-main">
        <h1>Introduction</h1>
        <p class="text">
            When observing a moving scene, humans don't process every detail equally. Our eyes dart around to moving objects, capture fine details, and skip over static backgrounds, efficiently understanding scenes by selectively attending to informative regions. This allows us to process high-FPS, high-resolution video streams in real time.
        </p>

        <p class="text">
            In contrast, modern video understanding models still process every pixel in every frame equally, <b>wasting computation due to spatiotemporal redundancy in videos</b>. Thus, these models cannot scale to <em>long-form</em> and <em>high-resolution</em> videos crucial for real-world applications due to computational cost.
        </p>

        <p class="text">
            We propose <b>AutoGaze, a lightweight model that attends to informative patches and removes redundant ones before passing into a ViT or MLLM</b>. AutoGaze perceives each frame and autoregressively selects a minimal set of multi-scale patches which, along with the selected patches from previous frames, can reconstruct the current frame.
        </p>
    </div>
    <br>
    <div class="container blog extra-large">
        <img src="assets/figures/autogaze/teaser_v14.png" style="width: 100%;">
        <p class="caption">
            <b>Left: </b> Existing MLLMs either <span id="naive-mllm-label">process all pixels which is inefficient</span>, or <span id="prune-mllm-label">prune tokens only in their LLMs, leaving ViTs the computational bottleneck</span>. In contrast, <span id="autogaze-label">AutoGaze eliminates redundant patches by up to 100× <em>before</em> ViTs, accelerating ViTs and MLLMs by up to 19×</span>.
        </p>
        <p class="caption">
            <b>Right: </b> This efficiency enables MLLMs with AutoGaze to <b>scale to 1K-frame, 4K-resolution videos and achieve superior performance</b> on HLVid, our new long, high-resolution video benchmark, surpassing prior MLLMs limited to short or low-resolution videos.
        </p>
    </div>

    <div class="container blog main">
        <h1>Architecture and Training</h1>
        <p class="text">
            Given an input video, AutoGaze selects a minimal set of patches (i.e., "gazing") which can reconstruct the video within some user-specified reconstruction loss threshold or patch budget.
            The model is made up of a convolutional encoder and autoregressive transformer decoder, totaling 3M parameters. The decoder's vocabulary includes patches from four scales, letting the decoder select different scales for regions with different level of detail and reducing patches while preserving reconstruction quality.
        </p>
    </div>

    <div class="container blog extra-large">
<!--        <h2>Training Pipeline</h2>-->
        <p class="text">
            AutoGaze is trained in two stages: we first pre-train with next-token prediction (NTP) on ground-truth gazing sequences, then further post-train AutoGaze using RL with reconstruction reward to discover gazing sequences with lower reconstruction loss.
        </p>
        <br>
        <img src="assets/figures/autogaze/architecture_v15.png" style="width: 100%;">
        <p class="caption">
            <b>Left & Middle: </b> Given a video, AutoGaze processes each frame and outputs indices of multi-scale patches based on the history of frames and selected patches. Once AutoGaze believes the previously-gazed patches are sufficient to reconstruct the current frame, it automatically stops gazing and moves to the next frame.
        </p>
        <p class="caption">
        <b>Right: </b> AutoGaze is trained in two stages: next-token-prediction pre-training on collected gazing sequences, and RL post-training with reconstruction reward.
        </p>
    </div>
    <br>
    <div class="container blog extra-large">
    <h1>Qualitative Results</h1>
        <p class="text">
            For each example below, we show the original video, gazed patches from two scales, and reconstructed video.
        </p>
        <p class="text">
            In general, AutoGaze can 1) focus on moving objects while removing redundancy in static regions <b>(a-e)</b>, 2) adapt to scene changes by selecting more patches <b>(c, e)</b>, and 3) distribute attention with different granularity based on detailedness <b>(c, f)</b>. This allows AutoGaze to select a small ratio of patches (gazing ratio) without much information loss, as reflected by the reconstruction quality.
        </p>
    <img src="assets/figures/autogaze/main_results.png" style="width: 100%;">
    </div>
    <br>
<!--    <div class="container blog main">-->
<!--        <h1>What is AutoGaze Paying Attention To?</h1>-->
<!--        <p class="text">-->
<!--            AutoGaze's efficiency comes from selecting only a small fraction of patches — but does it make principled decisions about which patches to select and at what scale?-->
<!--        </p>-->
<!--    </div>-->
<!--    <div class="container blog main two-column-figure">-->
<!--        <div class="text-column">-->
<!--            <h2>AutoGaze Targets Moving Patches</h2>-->
<!--            <p class="text">-->
<!--                Motion is a primary source of new information across video frames, and intuitively should be selected. We indeed find that across all scales, AutoGaze more frequently selects patches with higher optical flow.-->
<!--            </p>-->
<!--            <p class="text">-->
<!--                <b>Left: </b> AutoGaze uses coarser scales to capture higher optical flow.-->
<!--                <br>-->
<!--                <b>Right: </b> Across all scales, AutoGaze more frequently selects patches with higher optical flow. Error bars represent SEM.-->
<!--            </p>-->
<!--        </div>-->
<!--        <div class="figure-column">-->
<!--            <img src="assets/figures/autogaze/opticalflow_4.png">-->
<!--        </div>-->
<!--    </div>-->

<!--    <div class="container blog main two-column-figure">-->
<!--        <div class="text-column">-->
<!--            <h2>Gazing Scale Correlates with Patch Detail</h2>-->
<!--            <p class="text">-->
<!--                Regions with different levels of detail should be represented with different scales.-->
<!--            </p>-->
<!--            <p class="text">-->
<!--                <b>Left: </b> At finer scales, AutoGaze selects more detailed patches (measured as Laplacian variance).-->
<!--                <br>-->
<!--                <b>Right: </b> With increasing detail, AutoGaze uses finer scales (ρ = .12, p < 0.001). Error bars = SEM.-->
<!--            </p>-->
<!--        </div>-->
<!--        <div class="figure-column">-->
<!--            <img src="assets/figures/autogaze/texture_stats.png">-->
<!--        </div>-->
<!--    </div>-->

    <div class="container blog main">
        <h1>Generalization to Out-of-Distribution Videos</h1>
        <p class="text">
            AutoGaze successfully generalizes to unconventional scenarios including CCTV footage, robot videos, and videos with constantly swapping foreground objects. Across different style transfers, AutoGaze also maintains consistent gazing patterns, continuing to track changing regions.
        </p>
    </div>
        <div class="container blog extra-large">
            <img src="assets/figures/autogaze/autogaze_ood_semantics.png" style="width: 100%;">
        </div>

        <div class="container blog extra-large">
            <img src="assets/figures/autogaze/autogaze_ood_style.png" style="width: 100%;">
        </div>

    <br>
    <div class="container blog main two-column-figure">

        <div class="text-column">
            <h1>Generalization to Videos with Camera Motion</h1>
            <p class="text">
                AutoGaze also generalizes to videos with camera motion. Although the gazing pattern is less intuitive due to the global motion,
                the gazing ratio remains similar and the reconstruction quality is still high.
            </p>
        </div>
        <div class="text-column">
            <video controls src="./assets/figures/camera_motion.mp4"></video>
        </div>
    </div>
    <br>
    <div class="container blog main">
        <h1>Efficiency of ViTs and MLLMs with AutoGaze</h1>
        <h2>How many patches do we need?</h2>

        <p class="text">
            There is a trade-off between gazing ratio and reconstruction loss: videos with higher FPS or resolution need lower gazing ratio to reach the same reconstruction quality.
            We find that a reconstruction loss threshold of 0.7 usually leads to <b>less than 0.5% performance degradation across benchmarks while allowing videos to be represented with 4×-100× fewer patches</b>.
        </p>
    </div>

    <div class="container blog main">
        <img src="assets/figures/autogaze/loss_recon_cut.png" style="width: 70%">
    </div>

    <div class="container blog main">
        <h2>What speedups do we get?</h2>
        <p class="text">
            When using the gazing ratio required for a reconstruction loss of 0.7, AutoGaze achieves up to 19× and 10× speedup for ViTs and MLLMs respectively, enabling scaling to 4K resolution. See below for latency plots for ending one second of video with varying FPS and resolution.
        </p>
    </div>

    <div class="container blog extra-large">
        <div class="columns-2">
            <img src="assets/figures/autogaze/latency_vision.png">
            <img src="assets/figures/autogaze/latency_llm.png">
        </div>
    </div>

    <div class="container blog main">
        <h1>HLVid: High-Resolution, Long-Form Video Benchmark</h1>
        <p class="text">
            Although AutoGaze enables efficient understanding of long, high-resolution videos, <b>benchmarks to evaluate this capability are still missing</b>. We propose HLVid, the first long-form, high-resolution video QA benchmark featuring <b>300 QA pairs on 5-minute, 4K-resolution videos</b>. Each question is manually reviewed to ensure high resolution and understanding throughout the entire video is required.
        </p>
        <p class="text">
            <a href="">Check out and use HLVid on HuggingFace!</a>
        </p>

        <pre><code class="python">from datasets import load_dataset
ds = load_dataset("bfshi/HLVid")</code></pre>
    </div>
    <div class="container blog main">
        <h1>Performance on Video Benchmarks</h1>
        <p class="text">
            We scale NVILA-8B to 1K-frame 4K-resolution videos, demonstrating consistent improvements on various benchmarks and outperforming strong MLLMs such as Qwen2.5-VL. See <a href="#">the paper</a> for detailed quantitative results comparing with other MLLMs.
        </p>
    </div>

    <div class="container blog extra-large">
        <img src="assets/figures/autogaze/mllm_benchmarks.png" style="width: 100%;">
    </div>

    <div class="container blog main">
        <h1>Citation</h1>
        <p class="text">
            If you find AutoGaze useful for your research, please cite:
        </p>
<pre><code class="plaintext">{



}</code></pre>
    </div>

    <!-- Footer Page -->
    <footer style="background: linear-gradient(180deg, var(--gradient-end), var(--gradient-start));">
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>.
            </p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="assets/scripts/main.js"></script>
</body>
</html>